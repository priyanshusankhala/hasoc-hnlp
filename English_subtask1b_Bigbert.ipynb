{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1b_Bbert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyanshusankhala/hasoc-hnlp/blob/main/English_subtask1b_Bigbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-tiIkzjz_Rf",
        "outputId": "b79fc9cf-1e97-493e-e736-c6ee0897ee0c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTsXo-y-z-w_",
        "outputId": "1f8fa0a9-31c3-4119-e98b-52e5783f0e6a"
      },
      "source": [
        "cd /content/drive/MyDrive/HASOC/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HASOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp2TGirr0zF5",
        "outputId": "455b0d01-ef99-4b62-d331-e61fb13a234e"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mAnalysis\u001b[0m/            \u001b[01;34mData\u001b[0m/   predictionstrue.txt  \u001b[01;34m__pycache__\u001b[0m/\n",
            "\u001b[01;34mBert-large-cased-0\u001b[0m/  \u001b[01;34mmodel\u001b[0m/  predictions.txt      task1_baseline.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oPqyVB6NAJm"
      },
      "source": [
        "! pip install transformers\n",
        "! pip install neptune-client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfCzZxzgIhGJ"
      },
      "source": [
        "### New section1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4bv7JWzND-L"
      },
      "source": [
        "###############################################################\n",
        "\n",
        "# baseline code for task 1\n",
        "# using BERT-based classification\n",
        "# With the exception of the evaluation part \n",
        "# (which reflects the tasks evaluation code), \n",
        "# this code is taken from\n",
        "# https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "\n",
        "###############################################################\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import os\n",
        "import os.path\n",
        "from sklearn import metrics\n",
        "import neptune\n",
        "load_model = False\n",
        "save_model = False\n",
        "\n",
        "if load_model:\n",
        "    output_model_file = \"/content/drive/MyDrive/HASOC/model/my_own_model_file.bin\"\n",
        "    output_config_file = \"/content/drive/MyDrive/HASOC/model/my_own_config_file.bin\"\n",
        "    output_vocab_file = \"/content/drive/MyDrive/HASOC/model/my_own_vocab_file.bin\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc29Xi3Ymxnf"
      },
      "source": [
        "#run\n",
        "###############################################################\n",
        "\n",
        "# baseline code for task 1\n",
        "# using BERT-based classification\n",
        "# With the exception of the evaluation part \n",
        "# (which reflects the tasks evaluation code), \n",
        "# this code is taken from\n",
        "# https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "\n",
        "###############################################################\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import os\n",
        "import os.path\n",
        "from sklearn import metrics\n",
        "import neptune\n",
        "load_model = False\n",
        "save_model = True\n",
        "\n",
        "if load_model:\n",
        "    output_model_file = \"/content/drive/MyDrive/HASOC/model/my_own_model_file.bin\"\n",
        "    output_config_file = \"/content/drive/MyDrive/HASOC/model/my_own_config_file.bin\"\n",
        "    output_vocab_file = \"/content/drive/MyDrive/HASOC/model/my_own_vocab_file.bin\"\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/HASOC/Data/en_Hasoc2021_train.csv\", delimiter=',')\n",
        "# dev_df = pd.read_csv(\"/content/drive/MyDrive/HASOC/Data/en_Hasoc2021_train.csv\", delimiter=',', header=None, names=['id', 'sentence', 'label', 'alpha'])\n",
        "\n",
        "data.task_1 = data.task_1.str.replace('NOT','0')\n",
        "data.task_1 = data.task_1.str.replace('HOF','1')\n",
        "data.task_1 = data.task_1.astype(int)\n",
        "data.task_2 = data.task_2.str.replace('PRFN','0')\n",
        "data.task_2 = data.task_2.str.replace('OFFN','1')\n",
        "data.task_2 = data.task_2.str.replace('NONE','2')\n",
        "data.task_2 = data.task_2.str.replace('HATE','3')\n",
        "data.task_2 = data.task_2.astype(int)\n",
        "df2 = data.copy()\n",
        "df = df2.sample(frac=0.8, random_state=0)\n",
        "dev_df = df2.drop(df.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbZKl4Rs27fa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErEL293v2E2c"
      },
      "source": [
        "df2 = data.copy()\n",
        "df = df2.sample(frac=0.8, random_state=0)\n",
        "dev_df = df2.drop(df.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "_8hFYF-lpvt_",
        "outputId": "8bd223eb-51c1-4a13-e3cd-598125f4e374"
      },
      "source": [
        "#1 df.head()\n",
        "data.head()\n",
        "data.task_2.value_counts()\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data.task_2.hist()\n",
        "data['task_2'].value_counts(normalize=True) * 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    34.920635\n",
              "0    31.121520\n",
              "3    17.772574\n",
              "1    16.185272\n",
              "Name: task_2, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "wLnie2vABslG",
        "outputId": "e789a6ad-9500-4255-a9bb-04bf41bce71b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data.task_2.hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7cf75a0690>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8h2_drmFs8h",
        "outputId": "4456ecc5-8750-45f3-f8d0-af0d0cf3050b"
      },
      "source": [
        "data['task_2'].value_counts(normalize=True) * 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    34.920635\n",
              "0    31.121520\n",
              "3    17.772574\n",
              "1    16.185272\n",
              "Name: task_2, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjA66IbCp2on",
        "outputId": "ed07b1cf-2575-4dd7-ca70-662ca22ad443"
      },
      "source": [
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences: 3,074\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRecPUdMuD-j",
        "outputId": "521f64a8-dccd-4fd8-c7ca-e4aaa95ccf9a"
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.text.values\n",
        "labels = df.task_2.values\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 3, 0, ..., 2, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG5V2EKVPefV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzMsWsNruGfY"
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "dev_sentences = dev_df.text.values\n",
        "dev_labels = dev_df.task_2.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUGhRU-QHfmV",
        "outputId": "8bf0b3f2-f124-4453-f381-c609fbfdfe2d"
      },
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "if load_model:\n",
        "    tokenizer = BertTokenizer.from_pretrained(output_vocab_file, do_lower_case = False)\n",
        "else:\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-large-cased', do_lower_case=False)\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "dev_input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(str(sent),add_special_tokens = True,)\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# For every sentence...\n",
        "for sent in dev_sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(str(sent),add_special_tokens = True,)\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    dev_input_ids.append(encoded_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UbXwGvLHwef"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "l = [len(x) for x in input_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvPwBkhoIS8X",
        "outputId": "3a4fa205-e7e3-4174-aed4-2d5d4c389155"
      },
      "source": [
        "np.quantile(l, 0.98)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "110.0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "Jr9irbvfIAYY",
        "outputId": "1bfdd6a6-829b-473a-c323-3676aff9c9ed"
      },
      "source": [
        "np.histogram(l)\n",
        "plt.hist(l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1.106e+03, 1.364e+03, 5.590e+02, 4.200e+01, 1.000e+00, 0.000e+00,\n",
              "        1.000e+00, 0.000e+00, 0.000e+00, 1.000e+00]),\n",
              " array([  4. ,  41.2,  78.4, 115.6, 152.8, 190. , 227.2, 264.4, 301.6,\n",
              "        338.8, 376. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "YaQyxqbmN8Hi",
        "outputId": "408f1606-b463-4766-fc9b-94be26787fcb"
      },
      "source": [
        "df['task_2'].value_counts()[:].plot(kind='barh')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f7cf69120d0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39XrVlNM-k5D"
      },
      "source": [
        "# torch.save(model,'model.pth')\n",
        "# model = torch.load('model.pth')\n",
        "# Load the dataset into a pandas dataframe.\n",
        "def func_call_test(epoch_id):\n",
        "    test_df = pd.read_csv(\"/content/drive/MyDrive/HASOC/Data/en_Hasoc2021_test_task1.csv\", delimiter=',' , names=['id', 'sentence', 'label'])\n",
        "    # data.task_1 = data.task_1.str.replace('NOT','0')\n",
        "    # data.task_1 = data.task_1.str.replace('HOF','1')\n",
        "    # data.task_1 = data.task_1.astype(int)\n",
        "    test_sentences = test_df.sentence.values\n",
        "    test_labels = test_df.sentence.values\n",
        "    test_labels = test_df.label.values\n",
        "    # Report the number of sentences.\n",
        "    print('Number of test sentences: {:,}\\n'.format(test_df.shape[0]))\n",
        "    # /Check This\n",
        "    # Will this make sense now?\n",
        "    # Since the files are no longer loading with the gives headers. Think about it and give the necessary loading names\n",
        "    # /\n",
        "    # /test_sentences = test_df.sentence.values ##\n",
        "    # /test_labels = test_df.label.values\n",
        "    # / UNcomment/delete after done\n",
        "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "    test_input_ids = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in test_sentences:\n",
        "        # `encode` will:\n",
        "        #   (1) Tokenize the sentence.\n",
        "        #   (2) Prepend the `[CLS]` token to the start.\n",
        "        #   (3) Append the `[SEP]` token to the end.\n",
        "        #   (4) Map tokens to their IDs.\n",
        "        encoded_sent = tokenizer.encode(str(sent),add_special_tokens = True,)\n",
        "        \n",
        "        test_input_ids.append(encoded_sent)\n",
        "\n",
        "    # Pad our input tokens\n",
        "    test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, \n",
        "                              dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    pad_token = tokenizer.pad_token_id\n",
        "    # Create attention masks\n",
        "    test_attention_masks = []\n",
        "\n",
        "    # Create a mask of 1s for each token followed by 0s for padding\n",
        "    for seq in test_input_ids:\n",
        "      seq_mask = [int(i != pad_token) for i in seq]\n",
        "      test_attention_masks.append(seq_mask) \n",
        "\n",
        "    # Convert to tensors.\n",
        "    prediction_inputs = torch.tensor(test_input_ids).to(torch.int64)\n",
        "    prediction_masks = torch.tensor(test_attention_masks)\n",
        "    prediction_labels = torch.tensor(test_labels).to(torch.int64)\n",
        "\n",
        "    # Set the batch size.  \n",
        "    batch_size = 32  \n",
        "\n",
        "    # Create the DataLoader.\n",
        "    prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "    prediction_sampler = SequentialSampler(prediction_data)\n",
        "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "    # Prediction on test set\n",
        "\n",
        "    print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "    # Put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    predictions , true_labels = [], []\n",
        "   \n",
        "    # Predict \n",
        "    for batch in prediction_dataloader:\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      \n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      \n",
        "      # Telling the model not to compute or store gradients, saving memory and \n",
        "      # speeding up prediction\n",
        "      with torch.no_grad():\n",
        "          # Forward pass, calculate logit predictions\n",
        "          outputs = model(b_input_ids, token_type_ids=None, \n",
        "                          attention_mask=b_input_mask)\n",
        "\n",
        "      logits = outputs[0]\n",
        "\n",
        "      # Move logits and labels to CPU\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      label_ids = b_labels.to('cpu').numpy()\n",
        "      \n",
        "      # Store predictions and true labels\n",
        "      predictions.append(logits)\n",
        "      true_labels.append(label_ids)\n",
        "\n",
        "    print('    DONE.')\n",
        "\n",
        "    print('Positive samples: %d of %d (%.2f%%)' % (test_df.label.sum(), len(test_df.label), (test_df.label.sum() / len(test_df.label) * 100.0)))\n",
        "    with open('/content/drive/MyDrive/HASOC/Data/predictionstrue1_1b_bbert.csv', \"w\") as writer:\n",
        "        for i,line in enumerate(predictions):\n",
        "            writer.write(str(line) +\" \" +str(true_labels[i]) + \"\\n\")\n",
        "      \n",
        "    # Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "    print(flat_predictions)\n",
        "    # Combine the correct labels for each batch into a single list.\n",
        "    flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "    def evaluate(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Evaluate Precision, Recall, F1 scores between y_true and y_pred\n",
        "        If output_file is provided, scores are saved in this file otherwise printed to std output.\n",
        "        :param y_true: true labels\n",
        "        :param y_pred: predicted labels\n",
        "        :return: list of scores (F1, Recall, Precision, ExactMatch)\n",
        "        \"\"\"\n",
        "        \n",
        "        assert len(y_true) == len(y_pred)\n",
        "        precision, recall, f1, _ = metrics.precision_recall_fscore_support(y_true, y_pred, labels=[0, 1], average='weighted')\n",
        "        scores = [\n",
        "            \"F1: %f\\n\" % f1,\n",
        "            \"Recall: %f\\n\" % recall,\n",
        "            \"Precision: %f\\n\" % precision,\n",
        "            \"ExactMatch: %f\\n\" % -1.0\n",
        "        ]\n",
        "        for s in scores:\n",
        "            print(s, end='')\n",
        "\n",
        "    # Evaluate predictions    \n",
        "    evaluate(flat_true_labels, flat_predictions)\n",
        "\n",
        "    print('Writing predictions to file...')\n",
        "    with open(\"/content/drive/MyDrive/HASOC/Data/statout_1b_Bbert\"+str(epoch_i)+\".csv\", \"w\") as writer:\n",
        "      for line in flat_predictions:\n",
        "            writer.write(str(line) + \"\\n\")\n",
        "    #//Check this    \n",
        "   #// You had to interchange these two\n",
        "    # Save predictions to file\n",
        "    with open('/content/drive/MyDrive/HASOC/Data/predictions1_1b_Bbert.csv', \"w\") as writer:\n",
        "        for line in flat_predictions:\n",
        "            writer.write(str(line) + \"\\n\")\n",
        "            \n",
        "    print('Done writing predictions...')\n",
        "\n",
        "\n",
        "    if(save_model):\n",
        "        v = 0\n",
        "        folder_name = \"Bert-large-cased-\" + str(v)\n",
        "        if(os.path.exists(folder_name)):\n",
        "            v+=1\n",
        "            folder_name = \"Bert-large-uncased-\" + str(v)\n",
        "        else:\n",
        "            os.mkdir(folder_name)\n",
        "\n",
        "        # output_model_file = \"./{}/my_own_model_file.bin\".format(folder_name)\n",
        "        # output_config_file = \"./{}/my_own_config_file.bin\".format(folder_name)\n",
        "        # output_vocab_file = \"./{}/my_own_vocab_file.bin\".format(folder_name)\n",
        "        output_model_file = \"/content/drive/MyDrive/HASOC/model/my_own_model_file.bin\".format(model)\n",
        "        output_config_file = \"/content/drive/MyDrive/HASOC/model/my_own_config_file.bin\".format(model)\n",
        "        output_vocab_file = \"/content/drive/MyDrive/HASOC/model/my_own_vocab_file.bin\".format(model)\n",
        "\n",
        "        # Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
        "\n",
        "        # If we have a distributed model, save only the encapsulated model\n",
        "        # (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
        "        model_to_save = model.module if hasattr(model, 'module') else model\n",
        "\n",
        "        torch.save(model_to_save.state_dict(), output_model_file)\n",
        "        model_to_save.config.to_json_file(output_config_file)\n",
        "        tokenizer.save_vocabulary(output_vocab_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39tEOOkOBTiB"
      },
      "source": [
        "#func_call_test(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAJ720bpuloV",
        "outputId": "5c3b35c8-dbec-4289-f42c-6eb3b39a76ba"
      },
      "source": [
        "\n",
        "    \n",
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 120\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "dev_input_ids = pad_sequences(dev_input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')\n",
        "\n",
        "\n",
        "pad_token = tokenizer.pad_token_id\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "dev_attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id != pad_token) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)\n",
        "\n",
        "# For each sentence...\n",
        "for sent in dev_input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id != pad_token) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    dev_attention_masks.append(att_mask)\n",
        "    \n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs = input_ids \n",
        "validation_inputs = dev_input_ids\n",
        "train_labels = labels\n",
        "validation_labels = dev_labels\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks =  attention_masks\n",
        "validation_masks = dev_attention_masks\n",
        "                                             \n",
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs).to(torch.int64)\n",
        "validation_inputs = torch.tensor(validation_inputs).to(torch.int64)\n",
        "\n",
        "train_labels = torch.tensor(train_labels).to(torch.int64)\n",
        "validation_labels = torch.tensor(validation_labels).to(torch.int64)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "if load_model:\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top.\n",
        "    config = BertConfig.from_json_file(output_config_file)\n",
        "    model = BertForSequenceClassification(config)\n",
        "    state_dict = torch.load(output_model_file)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "else: \n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-large-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "        num_labels = 4, # The number of output labels--2 for binary classification.\n",
        "                        # You can increase this for multi-class tasks.   \n",
        "        output_attentions = False, # Whether the model returns attentions weights.\n",
        "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "    )\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "#model.cuda()\n",
        "print(device)\n",
        "\n",
        "\n",
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "print('==== Embedding Layer ====\\n')\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "    \n",
        "learning_rate = 2e-5\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "use_neptune = 1                                            \n",
        "if use_neptune:\n",
        "    api = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4NDRkYTliMS1lZGRjLTRhOWMtOGU0ZC04OTEzNzU1Y2E2Y2MifQ==\"\n",
        "    neptune.init(project_qualified_name='hnlp.hasoc/nlp', api_token=api)\n",
        "    PARAMS = {'lr': learning_rate,}\n",
        "    tags = ['bert']\n",
        "    neptune.create_experiment(\"First_exp\", tags=tags, params=PARAMS)\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "    \n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "    \n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "criterion = nn.CrossEntropyLoss(weight = torch.Tensor([1/10,2/10,1/10,2/10]).to(device))\n",
        "# For each epoch...\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "    #new\n",
        "    train_loss = []\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device).long()\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        logits = outputs[0]\n",
        "        loss = criterion(logits,b_labels)\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "        neptune.log_metric(\"Loss_Train\",loss.item())\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        #new\n",
        "        train_loss.append(loss.item())\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    #print(\"  training loss: {0:.2f}\".format(train_loss))\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        ##\n",
        "        predictions.append(logits)\n",
        "        true_labels.append(label_ids)\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "        neptune.log_metric(\"Acc_Val\",tmp_eval_accuracy)\n",
        "    \n",
        "        # Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "        # Combine the correct labels for each batch into a single list.\n",
        "    flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "        \n",
        "    def evaluate(y_true, y_pred):\n",
        "    \n",
        "        assert len(y_true) == len(y_pred)\n",
        "        precision, recall, f1, _ = metrics.precision_recall_fscore_support(y_true, y_pred, labels=[0, 1], average='weighted')\n",
        "        scores = [\n",
        "            \"F1: %f\\n\" % f1,\n",
        "            \"Recall: %f\\n\" % recall,\n",
        "            \"Precision: %f\\n\" % precision,\n",
        "            \"ExactMatch: %f\\n\" % -1.0\n",
        "        ]\n",
        "        for s in scores:\n",
        "            print(s, end='')\n",
        "        # Evaluate predictions    \n",
        "    evaluate(flat_true_labels, flat_predictions)\n",
        "    # Calculate the accuracy for this batch of test sentences.\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "    \n",
        "\n",
        "    func_call_test(epoch_i)\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Padding/truncating all sentences to 120 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "The BERT model has 393 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (28996, 1024)\n",
            "bert.embeddings.position_embeddings.weight               (512, 1024)\n",
            "bert.embeddings.token_type_embeddings.weight               (2, 1024)\n",
            "bert.embeddings.LayerNorm.weight                             (1024,)\n",
            "bert.embeddings.LayerNorm.bias                               (1024,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight        (1024, 1024)\n",
            "bert.encoder.layer.0.attention.self.query.bias               (1024,)\n",
            "bert.encoder.layer.0.attention.self.key.weight          (1024, 1024)\n",
            "bert.encoder.layer.0.attention.self.key.bias                 (1024,)\n",
            "bert.encoder.layer.0.attention.self.value.weight        (1024, 1024)\n",
            "bert.encoder.layer.0.attention.self.value.bias               (1024,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight      (1024, 1024)\n",
            "bert.encoder.layer.0.attention.output.dense.bias             (1024,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight       (1024,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias         (1024,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight          (4096, 1024)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (4096,)\n",
            "bert.encoder.layer.0.output.dense.weight                (1024, 4096)\n",
            "bert.encoder.layer.0.output.dense.bias                       (1024,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                 (1024,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                   (1024,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                (1024, 1024)\n",
            "bert.pooler.dense.bias                                       (1024,)\n",
            "classifier.weight                                          (4, 1024)\n",
            "classifier.bias                                                 (4,)\n",
            "https://app.neptune.ai/hnlp.hasoc/nlp/e/NLP-96\n",
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    385.    Elapsed: 0:00:52.\n",
            "  Batch    80  of    385.    Elapsed: 0:01:43.\n",
            "  Batch   120  of    385.    Elapsed: 0:02:34.\n",
            "  Batch   160  of    385.    Elapsed: 0:03:25.\n",
            "  Batch   200  of    385.    Elapsed: 0:04:15.\n",
            "  Batch   240  of    385.    Elapsed: 0:05:06.\n",
            "  Batch   280  of    385.    Elapsed: 0:05:57.\n",
            "  Batch   320  of    385.    Elapsed: 0:06:48.\n",
            "  Batch   360  of    385.    Elapsed: 0:07:39.\n",
            "\n",
            "  Average training loss: 1.08\n",
            "  Training epcoh took: 0:08:10\n",
            "\n",
            "Running Validation...\n",
            "F1: 0.691204\n",
            "Recall: 0.762040\n",
            "Precision: 0.633778\n",
            "ExactMatch: -1.000000\n",
            "  Accuracy: 0.66\n",
            "  Validation took: 0:08:50\n",
            "Number of test sentences: 1,282\n",
            "\n",
            "Predicting labels for 1,282 test sentences...\n",
            "    DONE.\n",
            "Positive samples: 0 of 1282 (0.00%)\n",
            "[2 2 2 ... 2 2 2]\n",
            "F1: 0.000000\n",
            "Recall: 0.000000\n",
            "Precision: 0.000000\n",
            "ExactMatch: -1.000000\n",
            "Writing predictions to file...\n",
            "Done writing predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    385.    Elapsed: 0:00:51.\n",
            "  Batch    80  of    385.    Elapsed: 0:01:42.\n",
            "  Batch   120  of    385.    Elapsed: 0:02:33.\n",
            "  Batch   160  of    385.    Elapsed: 0:03:24.\n",
            "  Batch   200  of    385.    Elapsed: 0:04:15.\n",
            "  Batch   240  of    385.    Elapsed: 0:05:06.\n",
            "  Batch   280  of    385.    Elapsed: 0:05:57.\n",
            "  Batch   320  of    385.    Elapsed: 0:06:48.\n",
            "  Batch   360  of    385.    Elapsed: 0:07:39.\n",
            "\n",
            "  Average training loss: 0.79\n",
            "  Training epcoh took: 0:08:10\n",
            "\n",
            "Running Validation...\n",
            "F1: 0.689034\n",
            "Recall: 0.722380\n",
            "Precision: 0.670886\n",
            "ExactMatch: -1.000000\n",
            "  Accuracy: 0.68\n",
            "  Validation took: 0:08:50\n",
            "Number of test sentences: 1,282\n",
            "\n",
            "Predicting labels for 1,282 test sentences...\n",
            "    DONE.\n",
            "Positive samples: 0 of 1282 (0.00%)\n",
            "[2 2 2 ... 2 2 2]\n",
            "F1: 0.000000\n",
            "Recall: 0.000000\n",
            "Precision: 0.000000\n",
            "ExactMatch: -1.000000\n",
            "Writing predictions to file...\n",
            "Done writing predictions...\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    385.    Elapsed: 0:00:51.\n",
            "  Batch    80  of    385.    Elapsed: 0:01:42.\n",
            "  Batch   120  of    385.    Elapsed: 0:02:33.\n",
            "  Batch   160  of    385.    Elapsed: 0:03:24.\n",
            "  Batch   200  of    385.    Elapsed: 0:04:15.\n",
            "  Batch   240  of    385.    Elapsed: 0:05:06.\n",
            "  Batch   280  of    385.    Elapsed: 0:05:57.\n",
            "  Batch   320  of    385.    Elapsed: 0:06:48.\n",
            "  Batch   360  of    385.    Elapsed: 0:07:39.\n",
            "\n",
            "  Average training loss: 0.51\n",
            "  Training epcoh took: 0:08:10\n",
            "\n",
            "Running Validation...\n",
            "F1: 0.684847\n",
            "Recall: 0.719547\n",
            "Precision: 0.653631\n",
            "ExactMatch: -1.000000\n",
            "  Accuracy: 0.67\n",
            "  Validation took: 0:08:50\n",
            "Number of test sentences: 1,282\n",
            "\n",
            "Predicting labels for 1,282 test sentences...\n",
            "    DONE.\n",
            "Positive samples: 0 of 1282 (0.00%)\n",
            "[2 2 2 ... 2 2 2]\n",
            "F1: 0.000000\n",
            "Recall: 0.000000\n",
            "Precision: 0.000000\n",
            "ExactMatch: -1.000000\n",
            "Writing predictions to file...\n",
            "Done writing predictions...\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    385.    Elapsed: 0:00:51.\n",
            "  Batch    80  of    385.    Elapsed: 0:01:42.\n",
            "  Batch   120  of    385.    Elapsed: 0:02:33.\n",
            "  Batch   160  of    385.    Elapsed: 0:03:24.\n",
            "  Batch   200  of    385.    Elapsed: 0:04:15.\n",
            "  Batch   240  of    385.    Elapsed: 0:05:06.\n",
            "  Batch   280  of    385.    Elapsed: 0:05:57.\n",
            "  Batch   320  of    385.    Elapsed: 0:06:48.\n",
            "  Batch   360  of    385.    Elapsed: 0:07:39.\n",
            "\n",
            "  Average training loss: 0.29\n",
            "  Training epcoh took: 0:08:10\n",
            "\n",
            "Running Validation...\n",
            "F1: 0.691058\n",
            "Recall: 0.705382\n",
            "Precision: 0.678576\n",
            "ExactMatch: -1.000000\n",
            "  Accuracy: 0.68\n",
            "  Validation took: 0:08:50\n",
            "Number of test sentences: 1,282\n",
            "\n",
            "Predicting labels for 1,282 test sentences...\n",
            "    DONE.\n",
            "Positive samples: 0 of 1282 (0.00%)\n",
            "[2 2 2 ... 2 2 2]\n",
            "F1: 0.000000\n",
            "Recall: 0.000000\n",
            "Precision: 0.000000\n",
            "ExactMatch: -1.000000\n",
            "Writing predictions to file...\n",
            "Done writing predictions...\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rD_565CeUr2_",
        "outputId": "8c3511fd-c108-4b75-8f54-a23f92c93a97"
      },
      "source": [
        "\n",
        "for epoch_i in range(epochs, epochs*2):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "    #new\n",
        "    train_loss = []\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device).long()\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        logits = outputs[0]\n",
        "        loss = criterion(logits,b_labels)\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "        neptune.log_metric(\"Loss_Train\",loss.item())\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        #new\n",
        "        train_loss.append(loss.item())\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    #print(\"  training loss: {0:.2f}\".format(train_loss))\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        ##\n",
        "        predictions.append(logits)\n",
        "        true_labels.append(label_ids)\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "        neptune.log_metric(\"Acc_Val\",tmp_eval_accuracy)\n",
        "    \n",
        "        # Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "        # Combine the correct labels for each batch into a single list.\n",
        "    flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "        \n",
        "    def evaluate(y_true, y_pred):\n",
        "    \n",
        "        assert len(y_true) == len(y_pred)\n",
        "        precision, recall, f1, _ = metrics.precision_recall_fscore_support(y_true, y_pred, labels=[0, 1], average='weighted')\n",
        "        scores = [\n",
        "            \"F1: %f\\n\" % f1,\n",
        "            \"Recall: %f\\n\" % recall,\n",
        "            \"Precision: %f\\n\" % precision,\n",
        "            \"ExactMatch: %f\\n\" % -1.0\n",
        "        ]\n",
        "        for s in scores:\n",
        "            print(s, end='')\n",
        "        # Evaluate predictions    \n",
        "    evaluate(flat_true_labels, flat_predictions)\n",
        "    # Calculate the accuracy for this batch of test sentences.\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "    \n",
        "\n",
        "    func_call_test(epoch_i)\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 5 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    385.    Elapsed: 0:00:51.\n",
            "  Batch    80  of    385.    Elapsed: 0:01:42.\n",
            "  Batch   120  of    385.    Elapsed: 0:02:33.\n",
            "  Batch   160  of    385.    Elapsed: 0:03:24.\n",
            "  Batch   200  of    385.    Elapsed: 0:04:14.\n",
            "  Batch   240  of    385.    Elapsed: 0:05:05.\n",
            "  Batch   280  of    385.    Elapsed: 0:05:56.\n",
            "  Batch   320  of    385.    Elapsed: 0:06:47.\n",
            "  Batch   360  of    385.    Elapsed: 0:07:38.\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:08:08\n",
            "\n",
            "Running Validation...\n",
            "F1: 0.691058\n",
            "Recall: 0.705382\n",
            "Precision: 0.678576\n",
            "ExactMatch: -1.000000\n",
            "  Accuracy: 0.68\n",
            "  Validation took: 0:08:48\n",
            "Number of test sentences: 1,282\n",
            "\n",
            "Predicting labels for 1,282 test sentences...\n",
            "    DONE.\n",
            "Positive samples: 0 of 1282 (0.00%)\n",
            "[2 2 2 ... 2 2 2]\n",
            "F1: 0.000000\n",
            "Recall: 0.000000\n",
            "Precision: 0.000000\n",
            "ExactMatch: -1.000000\n",
            "Writing predictions to file...\n",
            "Done writing predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 6 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    385.    Elapsed: 0:00:51.\n",
            "  Batch    80  of    385.    Elapsed: 0:01:41.\n",
            "  Batch   120  of    385.    Elapsed: 0:02:32.\n",
            "  Batch   160  of    385.    Elapsed: 0:03:22.\n",
            "  Batch   200  of    385.    Elapsed: 0:04:13.\n",
            "  Batch   240  of    385.    Elapsed: 0:05:03.\n",
            "  Batch   280  of    385.    Elapsed: 0:05:54.\n",
            "  Batch   320  of    385.    Elapsed: 0:06:44.\n",
            "  Batch   360  of    385.    Elapsed: 0:07:35.\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epcoh took: 0:08:06\n",
            "\n",
            "Running Validation...\n",
            "F1: 0.691058\n",
            "Recall: 0.705382\n",
            "Precision: 0.678576\n",
            "ExactMatch: -1.000000\n",
            "  Accuracy: 0.68\n",
            "  Validation took: 0:08:45\n",
            "Number of test sentences: 1,282\n",
            "\n",
            "Predicting labels for 1,282 test sentences...\n",
            "    DONE.\n",
            "Positive samples: 0 of 1282 (0.00%)\n",
            "[2 2 2 ... 2 2 2]\n",
            "F1: 0.000000\n",
            "Recall: 0.000000\n",
            "Precision: 0.000000\n",
            "ExactMatch: -1.000000\n",
            "Writing predictions to file...\n",
            "Done writing predictions...\n",
            "\n",
            "======== Epoch 7 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    385.    Elapsed: 0:00:51.\n",
            "  Batch    80  of    385.    Elapsed: 0:01:41.\n",
            "  Batch   120  of    385.    Elapsed: 0:02:32.\n",
            "  Batch   160  of    385.    Elapsed: 0:03:23.\n",
            "  Batch   200  of    385.    Elapsed: 0:04:13.\n",
            "  Batch   240  of    385.    Elapsed: 0:05:04.\n",
            "  Batch   280  of    385.    Elapsed: 0:05:54.\n",
            "  Batch   320  of    385.    Elapsed: 0:06:45.\n",
            "  Batch   360  of    385.    Elapsed: 0:07:35.\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epcoh took: 0:08:06\n",
            "\n",
            "Running Validation...\n",
            "F1: 0.691058\n",
            "Recall: 0.705382\n",
            "Precision: 0.678576\n",
            "ExactMatch: -1.000000\n",
            "  Accuracy: 0.68\n",
            "  Validation took: 0:08:46\n",
            "Number of test sentences: 1,282\n",
            "\n",
            "Predicting labels for 1,282 test sentences...\n",
            "    DONE.\n",
            "Positive samples: 0 of 1282 (0.00%)\n",
            "[2 2 2 ... 2 2 2]\n",
            "F1: 0.000000\n",
            "Recall: 0.000000\n",
            "Precision: 0.000000\n",
            "ExactMatch: -1.000000\n",
            "Writing predictions to file...\n",
            "Done writing predictions...\n",
            "\n",
            "======== Epoch 8 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    385.    Elapsed: 0:00:51.\n",
            "  Batch    80  of    385.    Elapsed: 0:01:41.\n",
            "  Batch   120  of    385.    Elapsed: 0:02:32.\n",
            "  Batch   160  of    385.    Elapsed: 0:03:23.\n",
            "  Batch   200  of    385.    Elapsed: 0:04:13.\n",
            "  Batch   240  of    385.    Elapsed: 0:05:04.\n",
            "  Batch   280  of    385.    Elapsed: 0:05:54.\n",
            "  Batch   320  of    385.    Elapsed: 0:06:45.\n",
            "  Batch   360  of    385.    Elapsed: 0:07:36.\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epcoh took: 0:08:07\n",
            "\n",
            "Running Validation...\n",
            "F1: 0.691058\n",
            "Recall: 0.705382\n",
            "Precision: 0.678576\n",
            "ExactMatch: -1.000000\n",
            "  Accuracy: 0.68\n",
            "  Validation took: 0:08:46\n",
            "Number of test sentences: 1,282\n",
            "\n",
            "Predicting labels for 1,282 test sentences...\n",
            "    DONE.\n",
            "Positive samples: 0 of 1282 (0.00%)\n",
            "[2 2 2 ... 2 2 2]\n",
            "F1: 0.000000\n",
            "Recall: 0.000000\n",
            "Precision: 0.000000\n",
            "ExactMatch: -1.000000\n",
            "Writing predictions to file...\n",
            "Done writing predictions...\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E_2V9fxRqX0",
        "outputId": "2b84ff60-44d8-4aef-b829-21816a9ce25b"
      },
      "source": [
        "flat_predictions[-5:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXawIH--QNXR",
        "outputId": "bb24f309-01d7-4e2c-e76c-e5f4fda19a8a"
      },
      "source": [
        "test_df = pd.read_csv(\"/content/drive/MyDrive/HASOC/Data/en_Hasoc2021_test_task1.csv\", delimiter=',' , names=['id', 'sentence', 'label'])\n",
        "# data.task_1 = data.task_1.str.replace('NOT','0')\n",
        "# data.task_1 = data.task_1.str.replace('HOF','1')\n",
        "# data.task_1 = data.task_1.astype(int)\n",
        "test_sentences = test_df.sentence.values\n",
        "test_labels = test_df.label.values\n",
        "# Report the number of sentences.\n",
        "print('Number of test sentences: {:,}\\n'.format(test_df.shape[0]))\n",
        "# /Check This\n",
        "# Will this make sense now?\n",
        "# Since the files are no longer loading with the gives headers. Think about it and give the necessary loading names\n",
        "# /\n",
        "# /test_sentences = test_df.sentence.values ##\n",
        "# /test_labels = test_df.label.values\n",
        "# / UNcomment/delete after done\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "test_input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in test_sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(str(sent),add_special_tokens = True,)\n",
        "    \n",
        "    test_input_ids.append(encoded_sent)\n",
        "\n",
        "# Pad our input tokens\n",
        "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, value=0,\n",
        "                            dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "pad_token = tokenizer.pad_token_id\n",
        "# Create attention masks\n",
        "test_attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in test_input_ids:\n",
        "    seq_mask = [int(i > pad_token) for i in seq]\n",
        "    test_attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "prediction_inputs = torch.tensor(test_input_ids).to(torch.int64)\n",
        "prediction_masks = torch.tensor(test_attention_masks)\n",
        "prediction_labels = torch.tensor(test_labels).to(torch.int64)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')\n",
        "\n",
        "print('Positive samples: %d of %d (%.2f%%)' % (test_df.label.sum(), len(test_df.label), (test_df.label.sum() / len(test_df.label) * 100.0)))\n",
        "with open('/content/drive/MyDrive/HASOC/Data/predictionstrue1_1b_bbert.csv', \"w\") as writer:\n",
        "    for i,line in enumerate(predictions):\n",
        "        writer.write(str(line) +\" \" +str(true_labels[i]) + \"\\n\")\n",
        "    \n",
        "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "print(flat_predictions)\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "def evaluate(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Evaluate Precision, Recall, F1 scores between y_true and y_pred\n",
        "    If output_file is provided, scores are saved in this file otherwise printed to std output.\n",
        "    :param y_true: true labels\n",
        "    :param y_pred: predicted labels\n",
        "    :return: list of scores (F1, Recall, Precision, ExactMatch)\n",
        "    \"\"\"\n",
        "    \n",
        "    assert len(y_true) == len(y_pred)\n",
        "    precision, recall, f1, _ = metrics.precision_recall_fscore_support(y_true, y_pred, labels=[0, 1], average='weighted')\n",
        "    scores = [\n",
        "        \"F1: %f\\n\" % f1,\n",
        "        \"Recall: %f\\n\" % recall,\n",
        "        \"Precision: %f\\n\" % precision,\n",
        "        \"ExactMatch: %f\\n\" % -1.0\n",
        "    ]\n",
        "    for s in scores:\n",
        "        print(s, end='')\n",
        "\n",
        "# Evaluate predictions    \n",
        "evaluate(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Writing predictions to file...')\n",
        "with open(\"/content/drive/MyDrive/HASOC/Data/statout_1b_Bbert_check\"+str(epoch_i)+\".csv\", \"w\") as writer:\n",
        "    for line in flat_predictions:\n",
        "        writer.write(str(line) + \"\\n\")\n",
        "#//Check this    \n",
        "#// You had to interchange these two\n",
        "# Save predictions to file\n",
        "with open('/content/drive/MyDrive/HASOC/Data/predictions1_1b_Bbert.csv', \"w\") as writer:\n",
        "    for line in flat_predictions:\n",
        "        writer.write(str(line) + \"\\n\")\n",
        "        \n",
        "print('Done writing predictions...')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test sentences: 1,282\n",
            "\n",
            "Predicting labels for 1,282 test sentences...\n",
            "    DONE.\n",
            "Positive samples: 0 of 1282 (0.00%)\n",
            "[2 2 0 ... 0 0 0]\n",
            "F1: 0.000000\n",
            "Recall: 0.000000\n",
            "Precision: 0.000000\n",
            "ExactMatch: -1.000000\n",
            "Writing predictions to file...\n",
            "Done writing predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "ndcoKQp2QzF2",
        "outputId": "e6593390-91ce-4739-86a4-fdbad3f89667"
      },
      "source": [
        "f1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_id</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60c5d7495659ea5e55df0546</td>\n",
              "      <td>hi_hasoc_2021_5</td>\n",
              "      <td>@hemantmkpandya @news24tvchannel @Aloksharmaai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60c5d7495659ea5e55df0591</td>\n",
              "      <td>hi_hasoc_2021_7</td>\n",
              "      <td>वोडाफोन ने एक कुत्ता पाला था बहुत फेमस हुआ   फ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60c5d7495659ea5e55df0622</td>\n",
              "      <td>hi_hasoc_2021_12</td>\n",
              "      <td>18-18 घंटे दीमक ने जाकर 70 साल के मज़बूत पेड़ ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60c5d7495659ea5e55df0666</td>\n",
              "      <td>hi_hasoc_2021_13</td>\n",
              "      <td>@dmfatehpur हमारे ग्राम पंचायत सिधांव जिला फते...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60c5d7495659ea5e55df067b</td>\n",
              "      <td>hi_hasoc_2021_15</td>\n",
              "      <td>यह मुझे चैन क्यों नहीं पड़ता एक ही शख़्स था जह...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1527</th>\n",
              "      <td>60c5d7495659ea5e55df18d6</td>\n",
              "      <td>hi_hasoc_2021_6116</td>\n",
              "      <td>@AcharyaPramodk @yadavakhilesh अंध भक्तो का वो...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1528</th>\n",
              "      <td>60c5d7495659ea5e55df18da</td>\n",
              "      <td>hi_hasoc_2021_6117</td>\n",
              "      <td>बंगाल में पिछले 3 दिनों में हुई कुछ हत्याओं पर...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1529</th>\n",
              "      <td>60c5d7495659ea5e55df191e</td>\n",
              "      <td>hi_hasoc_2021_6123</td>\n",
              "      <td>@Sohel__AK @ali_manihaar मुंशी प्रेमचंद ने खड्...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1530</th>\n",
              "      <td>60c5d7495659ea5e55df192f</td>\n",
              "      <td>hi_hasoc_2021_6124</td>\n",
              "      <td>लोगों को पेट्रोल सस्ता दो,   चाहें फिर देश को ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1531</th>\n",
              "      <td>60c5d7495659ea5e55df1981</td>\n",
              "      <td>hi_hasoc_2021_6126</td>\n",
              "      <td>@JPNadda नड्डा जी आतंकी को निंदा की नहीं डंडा ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1532 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                           _id  ...                                               text\n",
              "0     60c5d7495659ea5e55df0546  ...  @hemantmkpandya @news24tvchannel @Aloksharmaai...\n",
              "1     60c5d7495659ea5e55df0591  ...  वोडाफोन ने एक कुत्ता पाला था बहुत फेमस हुआ   फ...\n",
              "2     60c5d7495659ea5e55df0622  ...  18-18 घंटे दीमक ने जाकर 70 साल के मज़बूत पेड़ ...\n",
              "3     60c5d7495659ea5e55df0666  ...  @dmfatehpur हमारे ग्राम पंचायत सिधांव जिला फते...\n",
              "4     60c5d7495659ea5e55df067b  ...  यह मुझे चैन क्यों नहीं पड़ता एक ही शख़्स था जह...\n",
              "...                        ...  ...                                                ...\n",
              "1527  60c5d7495659ea5e55df18d6  ...  @AcharyaPramodk @yadavakhilesh अंध भक्तो का वो...\n",
              "1528  60c5d7495659ea5e55df18da  ...  बंगाल में पिछले 3 दिनों में हुई कुछ हत्याओं पर...\n",
              "1529  60c5d7495659ea5e55df191e  ...  @Sohel__AK @ali_manihaar मुंशी प्रेमचंद ने खड्...\n",
              "1530  60c5d7495659ea5e55df192f  ...  लोगों को पेट्रोल सस्ता दो,   चाहें फिर देश को ...\n",
              "1531  60c5d7495659ea5e55df1981  ...  @JPNadda नड्डा जी आतंकी को निंदा की नहीं डंडा ...\n",
              "\n",
              "[1532 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjNF1jq9nf5o"
      },
      "source": [
        "f1 = pd.read_csv(\"/content/drive/MyDrive/HASOC/Data/hi_Hasoc2021_test_task1.csv\")\n",
        "f2 = pd.read_csv(\"/content/drive/MyDrive/HASOC/Data/statout_hi_bert_task2_3.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh_wA_u6Fqsg",
        "outputId": "b960efff-d83b-4fce-bf84-96a6acfc96b0"
      },
      "source": [
        "f2.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1\n",
              "2    730\n",
              "3    449\n",
              "1    310\n",
              "0     43\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zysAZmUZPPgP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "ab8ce699-d872-4ceb-9751-84c41cfa8600"
      },
      "source": [
        "f1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_id</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60c5d7495659ea5e55df0546</td>\n",
              "      <td>hi_hasoc_2021_5</td>\n",
              "      <td>@hemantmkpandya @news24tvchannel @Aloksharmaai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60c5d7495659ea5e55df0591</td>\n",
              "      <td>hi_hasoc_2021_7</td>\n",
              "      <td>वोडाफोन ने एक कुत्ता पाला था बहुत फेमस हुआ   फ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60c5d7495659ea5e55df0622</td>\n",
              "      <td>hi_hasoc_2021_12</td>\n",
              "      <td>18-18 घंटे दीमक ने जाकर 70 साल के मज़बूत पेड़ ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60c5d7495659ea5e55df0666</td>\n",
              "      <td>hi_hasoc_2021_13</td>\n",
              "      <td>@dmfatehpur हमारे ग्राम पंचायत सिधांव जिला फते...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60c5d7495659ea5e55df067b</td>\n",
              "      <td>hi_hasoc_2021_15</td>\n",
              "      <td>यह मुझे चैन क्यों नहीं पड़ता एक ही शख़्स था जह...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1527</th>\n",
              "      <td>60c5d7495659ea5e55df18d6</td>\n",
              "      <td>hi_hasoc_2021_6116</td>\n",
              "      <td>@AcharyaPramodk @yadavakhilesh अंध भक्तो का वो...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1528</th>\n",
              "      <td>60c5d7495659ea5e55df18da</td>\n",
              "      <td>hi_hasoc_2021_6117</td>\n",
              "      <td>बंगाल में पिछले 3 दिनों में हुई कुछ हत्याओं पर...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1529</th>\n",
              "      <td>60c5d7495659ea5e55df191e</td>\n",
              "      <td>hi_hasoc_2021_6123</td>\n",
              "      <td>@Sohel__AK @ali_manihaar मुंशी प्रेमचंद ने खड्...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1530</th>\n",
              "      <td>60c5d7495659ea5e55df192f</td>\n",
              "      <td>hi_hasoc_2021_6124</td>\n",
              "      <td>लोगों को पेट्रोल सस्ता दो,   चाहें फिर देश को ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1531</th>\n",
              "      <td>60c5d7495659ea5e55df1981</td>\n",
              "      <td>hi_hasoc_2021_6126</td>\n",
              "      <td>@JPNadda नड्डा जी आतंकी को निंदा की नहीं डंडा ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1532 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                           _id  ...                                               text\n",
              "0     60c5d7495659ea5e55df0546  ...  @hemantmkpandya @news24tvchannel @Aloksharmaai...\n",
              "1     60c5d7495659ea5e55df0591  ...  वोडाफोन ने एक कुत्ता पाला था बहुत फेमस हुआ   फ...\n",
              "2     60c5d7495659ea5e55df0622  ...  18-18 घंटे दीमक ने जाकर 70 साल के मज़बूत पेड़ ...\n",
              "3     60c5d7495659ea5e55df0666  ...  @dmfatehpur हमारे ग्राम पंचायत सिधांव जिला फते...\n",
              "4     60c5d7495659ea5e55df067b  ...  यह मुझे चैन क्यों नहीं पड़ता एक ही शख़्स था जह...\n",
              "...                        ...  ...                                                ...\n",
              "1527  60c5d7495659ea5e55df18d6  ...  @AcharyaPramodk @yadavakhilesh अंध भक्तो का वो...\n",
              "1528  60c5d7495659ea5e55df18da  ...  बंगाल में पिछले 3 दिनों में हुई कुछ हत्याओं पर...\n",
              "1529  60c5d7495659ea5e55df191e  ...  @Sohel__AK @ali_manihaar मुंशी प्रेमचंद ने खड्...\n",
              "1530  60c5d7495659ea5e55df192f  ...  लोगों को पेट्रोल सस्ता दो,   चाहें फिर देश को ...\n",
              "1531  60c5d7495659ea5e55df1981  ...  @JPNadda नड्डा जी आतंकी को निंदा की नहीं डंडा ...\n",
              "\n",
              "[1532 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "nLNsDOI2P_rE",
        "outputId": "a6aa77ca-cba7-439f-bc29-de66dab1a0c3"
      },
      "source": [
        "f2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1527</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1528</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1529</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1530</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1531</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1532 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      1\n",
              "0     1\n",
              "1     0\n",
              "2     2\n",
              "3     1\n",
              "4     2\n",
              "...  ..\n",
              "1527  1\n",
              "1528  1\n",
              "1529  1\n",
              "1530  2\n",
              "1531  3\n",
              "\n",
              "[1532 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpjFCQX9QOhl"
      },
      "source": [
        "f1['label'] = f2['1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpqruFAGQG7B"
      },
      "source": [
        "f1 = f1.drop(['text','_id'],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "xRuDAqo0SKme",
        "outputId": "38000194-7803-4959-c047-cc30ef199fd9"
      },
      "source": [
        "f1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hi_hasoc_2021_5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hi_hasoc_2021_7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hi_hasoc_2021_12</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hi_hasoc_2021_13</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hi_hasoc_2021_15</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1527</th>\n",
              "      <td>hi_hasoc_2021_6116</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1528</th>\n",
              "      <td>hi_hasoc_2021_6117</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1529</th>\n",
              "      <td>hi_hasoc_2021_6123</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1530</th>\n",
              "      <td>hi_hasoc_2021_6124</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1531</th>\n",
              "      <td>hi_hasoc_2021_6126</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1532 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                tweet_id  label\n",
              "0        hi_hasoc_2021_5      1\n",
              "1        hi_hasoc_2021_7      0\n",
              "2       hi_hasoc_2021_12      2\n",
              "3       hi_hasoc_2021_13      1\n",
              "4       hi_hasoc_2021_15      2\n",
              "...                  ...    ...\n",
              "1527  hi_hasoc_2021_6116      1\n",
              "1528  hi_hasoc_2021_6117      1\n",
              "1529  hi_hasoc_2021_6123      1\n",
              "1530  hi_hasoc_2021_6124      2\n",
              "1531  hi_hasoc_2021_6126      3\n",
              "\n",
              "[1532 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9tHOqITPVyF"
      },
      "source": [
        "f1.columns = [\"id\",\"label\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FA0E_QYEPWL3"
      },
      "source": [
        "\n",
        "f1.label = f1.label.replace(0,'PRFN')\n",
        "f1.label = f1.label.replace(1,'OFFN')\n",
        "f1.label = f1.label.replace(2,'NONE')\n",
        "f1.label = f1.label.replace(3,'HATE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0ObX_x4P_i_"
      },
      "source": [
        "f1.to_csv(\"/content/drive/MyDrive/HASOC/Data/task1b_hi_bigbert.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZGVufYFQARf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "57c3fa1a-e515-4c73-bebb-b3d3d6aa1a89"
      },
      "source": [
        "f1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hi_hasoc_2021_5</td>\n",
              "      <td>OFFN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hi_hasoc_2021_7</td>\n",
              "      <td>PRFN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hi_hasoc_2021_12</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hi_hasoc_2021_13</td>\n",
              "      <td>OFFN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hi_hasoc_2021_15</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1527</th>\n",
              "      <td>hi_hasoc_2021_6116</td>\n",
              "      <td>OFFN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1528</th>\n",
              "      <td>hi_hasoc_2021_6117</td>\n",
              "      <td>OFFN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1529</th>\n",
              "      <td>hi_hasoc_2021_6123</td>\n",
              "      <td>OFFN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1530</th>\n",
              "      <td>hi_hasoc_2021_6124</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1531</th>\n",
              "      <td>hi_hasoc_2021_6126</td>\n",
              "      <td>HATE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1532 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      id label\n",
              "0        hi_hasoc_2021_5  OFFN\n",
              "1        hi_hasoc_2021_7  PRFN\n",
              "2       hi_hasoc_2021_12  NONE\n",
              "3       hi_hasoc_2021_13  OFFN\n",
              "4       hi_hasoc_2021_15  NONE\n",
              "...                  ...   ...\n",
              "1527  hi_hasoc_2021_6116  OFFN\n",
              "1528  hi_hasoc_2021_6117  OFFN\n",
              "1529  hi_hasoc_2021_6123  OFFN\n",
              "1530  hi_hasoc_2021_6124  NONE\n",
              "1531  hi_hasoc_2021_6126  HATE\n",
              "\n",
              "[1532 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeYzCdeCTNK6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}