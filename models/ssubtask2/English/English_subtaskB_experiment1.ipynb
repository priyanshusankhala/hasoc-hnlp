{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English HASOC",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyanshusankhala/hasoc-hnlp/blob/main/English_subtaskB_experiment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt-wIUUss6Y_"
      },
      "source": [
        "## Importing libraires"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-tiIkzjz_Rf",
        "outputId": "ce7587b5-57d0-488a-aa0d-a9a48a11d2af"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTsXo-y-z-w_",
        "outputId": "1e7c43ef-e34f-449b-c7d4-99eace9a0892"
      },
      "source": [
        "cd /content/drive/MyDrive/HASOC/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HASOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp2TGirr0zF5",
        "outputId": "8e817264-c21e-4064-da13-f81caf36a04e"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mAnalysis\u001b[0m/            \u001b[01;34mmodel\u001b[0m/               \u001b[01;34m__pycache__\u001b[0m/\n",
            "\u001b[01;34mBert-large-cased-0\u001b[0m/  predictionstrue.txt  task1_baseline.py\n",
            "\u001b[01;34mData\u001b[0m/                predictions.txt      train_hi_sent.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oPqyVB6NAJm",
        "outputId": "85abce9d-afe7-4634-ef40-3bb0f6da323d"
      },
      "source": [
        "! pip install transformers\n",
        "! pip install neptune-client"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 57.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 61.6 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 36.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n",
            "Collecting neptune-client\n",
            "  Downloading neptune-client-0.10.6.tar.gz (253 kB)\n",
            "\u001b[K     |████████████████████████████████| 253 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting bravado\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 37.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.1.5)\n",
            "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (7.1.2)\n",
            "Collecting PyJWT\n",
            "  Downloading PyJWT-2.1.0-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.15.0)\n",
            "Collecting websocket-client!=1.0.0,>=0.35.0\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting GitPython>=2.0.8\n",
            "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 41.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from neptune-client) (21.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from neptune-client) (1.24.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from GitPython>=2.0.8->neptune-client) (3.7.4.3)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->neptune-client) (2021.5.30)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (5.4.1)\n",
            "Collecting bravado-core>=5.16.1\n",
            "  Downloading bravado_core-5.17.0-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (2.8.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from bravado->neptune-client) (1.0.2)\n",
            "Collecting monotonic\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting simplejson\n",
            "  Downloading simplejson-3.17.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (129 kB)\n",
            "\u001b[K     |████████████████████████████████| 129 kB 53.0 MB/s \n",
            "\u001b[?25hCollecting swagger-spec-validator>=2.0.1\n",
            "  Downloading swagger_spec_validator-2.7.3-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2018.9)\n",
            "Collecting jsonref\n",
            "  Downloading jsonref-0.2-py3-none-any.whl (9.3 kB)\n",
            "Requirement already satisfied: jsonschema[format]>=2.5.1 in /usr/local/lib/python3.7/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client) (2.6.0)\n",
            "Collecting webcolors\n",
            "  Downloading webcolors-1.11.1-py3-none-any.whl (9.9 kB)\n",
            "Collecting rfc3987\n",
            "  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n",
            "Collecting strict-rfc3339\n",
            "  Downloading strict-rfc3339-0.7.tar.gz (17 kB)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->neptune-client) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->neptune-client) (1.19.5)\n",
            "Building wheels for collected packages: neptune-client, future, strict-rfc3339\n",
            "  Building wheel for neptune-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neptune-client: filename=neptune_client-0.10.6-py2.py3-none-any.whl size=433273 sha256=6c51aa55911d28a243709bdc96a1c14bca1e516c4079de35a9f2ba190e702b22\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/e8/66/634ef05538aef9821051b8d6acdafebecd3301ad390f8f2365\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=3b9a408136da7217ed3f80963b377151600fc543faf111a762d7ca304d7138c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for strict-rfc3339 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for strict-rfc3339: filename=strict_rfc3339-0.7-py3-none-any.whl size=18149 sha256=b44aeed4c31038980caf65b8e09d95de5621a4398389af7cf06a524f0e9f5e8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/1d/9f/2a74caecb81b8beb9a4fbe1754203d4b7cf42ef5d39e0d2311\n",
            "Successfully built neptune-client future strict-rfc3339\n",
            "Installing collected packages: webcolors, strict-rfc3339, rfc3987, swagger-spec-validator, smmap, simplejson, jsonref, monotonic, gitdb, bravado-core, websocket-client, PyJWT, GitPython, future, bravado, neptune-client\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed GitPython-3.1.18 PyJWT-2.1.0 bravado-11.0.3 bravado-core-5.17.0 future-0.18.2 gitdb-4.0.7 jsonref-0.2 monotonic-1.6 neptune-client-0.10.6 rfc3987-1.3.8 simplejson-3.17.4 smmap-4.0.0 strict-rfc3339-0.7 swagger-spec-validator-2.7.3 webcolors-1.11.1 websocket-client-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4bv7JWzND-L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "c4e0f779-d37a-4c31-f39d-944dde23b2a0"
      },
      "source": [
        "###############################################################\n",
        "\n",
        "# baseline code for task 1\n",
        "# using BERT-based classification\n",
        "# With the exception of the evaluation part \n",
        "# (which reflects the tasks evaluation code), \n",
        "# this code is taken from\n",
        "# https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "\n",
        "###############################################################\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import os\n",
        "import os.path\n",
        "from sklearn import metrics\n",
        "import neptune\n",
        "load_model = False\n",
        "save_model = False\n",
        "\n",
        "if load_model:\n",
        "    output_model_file = \"/content/drive/MyDrive/HASOC/model/my_own_model_file.bin\"\n",
        "    output_config_file = \"/content/drive/MyDrive/HASOC/model/my_own_config_file.bin\"\n",
        "    output_vocab_file = \"/content/drive/MyDrive/HASOC/model/my_own_vocab_file.bin\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-10f324925142>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzzcgC2ntFsm"
      },
      "source": [
        "Pre-proessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc29Xi3Ymxnf"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset into a pandas dataframe.\n",
        "#data = pd.read_csv(\"/content/drive/MyDrive/HASOC/Data/1B_English_GOLD.csv\", delimiter=',')\n",
        "data = pd.ExcelFile(\"/content/drive/MyDrive/HASOC/Data/1B_English_GOLD.xlsx\")\n",
        "# dev_df = pd.read_csv(\"/content/drive/MyDrive/HASOC/Data/en_Hasoc2021_train.csv\", delimiter=',', header=None, names=['id', 'sentence', 'label', 'alpha'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbZKl4Rs27fa"
      },
      "source": [
        "data.task_1 = data.task_1.str.replace('NOT','0')\n",
        "data.task_1 = data.task_1.str.replace('HOF','1')\n",
        "data.task_1 = data.task_1.astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErEL293v2E2c"
      },
      "source": [
        "df2 = data.copy()\n",
        "df = df2.sample(frac=0.8, random_state=0)\n",
        "dev_df = df2.drop(df.index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8hFYF-lpvt_"
      },
      "source": [
        "#1 df.head()\n",
        "data.head()\n",
        "#data.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "wLnie2vABslG",
        "outputId": "fcb6d804-f785-4317-bdc9-9cc453acafa0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data.task_2.hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f79bf430750>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8h2_drmFs8h",
        "outputId": "04c3cf39-3fd8-49c9-8ea0-5e394ed77a6d"
      },
      "source": [
        "data['task_2'].value_counts(normalize=True) * 100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NONE    34.920635\n",
              "PRFN    31.121520\n",
              "HATE    17.772574\n",
              "OFFN    16.185272\n",
              "Name: task_2, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjA66IbCp2on",
        "outputId": "9976dd62-8d5e-4302-e56e-aa4890d37c7e"
      },
      "source": [
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training sentences: 3,074\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRecPUdMuD-j",
        "outputId": "c961d3fd-5a2c-449e-9b24-2e16e8bb6a05"
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.text.values\n",
        "labels = df.task_1.values\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 1, 1, ..., 0, 0, 1])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzMsWsNruGfY"
      },
      "source": [
        "# Get the lists of sentences and their labels.\n",
        "dev_sentences = dev_df.text.values\n",
        "dev_labels = dev_df.task_1.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4FkXq0utJG-"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "jUGhRU-QHfmV",
        "outputId": "a06c227f-7f23-4915-999e-5be23db14033"
      },
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "if load_model:\n",
        "    tokenizer = BertTokenizer.from_pretrained(output_vocab_file, do_lower_case = False)\n",
        "else:\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "dev_input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(str(sent),add_special_tokens = True,)\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# For every sentence...\n",
        "for sent in dev_sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(str(sent),add_special_tokens = True,)\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    dev_input_ids.append(encoded_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/wk/y1l01d694jv117v9yn1mrtwm0000gn/T/ipykernel_37146/2407823954.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the BERT tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading BERT tokenizer...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_vocab_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UbXwGvLHwef"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "l = [len(x) for x in input_ids]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvPwBkhoIS8X",
        "outputId": "840d0b94-02cd-4a25-8dc3-377c492eea42"
      },
      "source": [
        "np.quantile(l, 0.98)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "110.0"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "Jr9irbvfIAYY",
        "outputId": "0190fba4-195f-4747-eb88-0d05c5ee3195"
      },
      "source": [
        "np.histogram(l)\n",
        "plt.hist(l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([1.106e+03, 1.364e+03, 5.590e+02, 4.200e+01, 1.000e+00, 0.000e+00,\n",
              "        1.000e+00, 0.000e+00, 0.000e+00, 1.000e+00]),\n",
              " array([  4. ,  41.2,  78.4, 115.6, 152.8, 190. , 227.2, 264.4, 301.6,\n",
              "        338.8, 376. ]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "YaQyxqbmN8Hi",
        "outputId": "573c5d1b-472b-487e-cea1-f7c252d2fe6f"
      },
      "source": [
        "df['task_1'].value_counts()[:].plot(kind='barh')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f92d6e3d790>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 266,
          "referenced_widgets": [
            "833adc7f4a1a459d85ddac9154485b93"
          ]
        },
        "id": "XAJ720bpuloV",
        "outputId": "b628a331-1f7b-4700-c254-5bbe253c19b9"
      },
      "source": [
        "\n",
        "    \n",
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 120\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "dev_input_ids = pad_sequences(dev_input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "dev_attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)\n",
        "\n",
        "# For each sentence...\n",
        "for sent in dev_input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    dev_attention_masks.append(att_mask)\n",
        "    \n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs = input_ids \n",
        "validation_inputs = dev_input_ids\n",
        "train_labels = labels\n",
        "validation_labels = dev_labels\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks =  attention_masks\n",
        "validation_masks = dev_attention_masks\n",
        "                                             \n",
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs).to(torch.int64)\n",
        "validation_inputs = torch.tensor(validation_inputs).to(torch.int64)\n",
        "\n",
        "train_labels = torch.tensor(train_labels).to(torch.int64)\n",
        "validation_labels = torch.tensor(validation_labels).to(torch.int64)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "if load_model:\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top.\n",
        "    config = BertConfig.from_json_file(output_config_file)\n",
        "    model = BertForSequenceClassification(config)\n",
        "    state_dict = torch.load(output_model_file)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "else: \n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "        num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                        # You can increase this for multi-class tasks.   \n",
        "        output_attentions = False, # Whether the model returns attentions weights.\n",
        "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "    )\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "#model.cuda()\n",
        "print(device)\n",
        "\n",
        "\n",
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "print('==== Embedding Layer ====\\n')\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "    \n",
        "learning_rate = 2e-5\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "use_neptune = 1                                            \n",
        "if use_neptune:\n",
        "    api = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4NDRkYTliMS1lZGRjLTRhOWMtOGU0ZC04OTEzNzU1Y2E2Y2MifQ==\"\n",
        "    neptune.init(project_qualified_name='hnlp.hasoc/nlp', api_token=api)\n",
        "    PARAMS = {'lr': learning_rate,}\n",
        "    tags = ['bert']\n",
        "    neptune.create_experiment(\"First_exp\", tags=tags, params=PARAMS)\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "    \n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "    \n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "criterion = nn.CrossEntropyLoss(weight = torch.Tensor([2/3,1/3]).to(device))\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "    #new\n",
        "    train_loss = []\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device).long()\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        logits = outputs[0]\n",
        "        loss = criterion(logits,b_labels)\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "        neptune.log_metric(\"Loss_Train\",loss.item())\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        #new\n",
        "        train_loss.append(loss.item())\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  training loss: {0:.2f}\".format(train_loss))\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "        neptune.log_metric(\"Acc_Val\",tmp_eval_accuracy)\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "    func_call_test(epoch_i)\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 120 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "833adc7f4a1a459d85ddac9154485b93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (28996, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Info (NVML): Driver Not Loaded. GPU usage metrics may not be reported. For more information, see https://docs-legacy.neptune.ai/logging-and-managing-experiment-results/logging-experiment-data.html#hardware-consumption \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://app.neptune.ai/hnlp.hasoc/nlp/e/NLP-4\n",
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch    40  of    385.    Elapsed: 0:06:58.\n",
            "  Batch    80  of    385.    Elapsed: 0:13:46.\n",
            "  Batch   120  of    385.    Elapsed: 0:20:36.\n",
            "  Batch   160  of    385.    Elapsed: 0:27:32.\n",
            "  Batch   200  of    385.    Elapsed: 0:34:15.\n",
            "  Batch   240  of    385.    Elapsed: 0:40:55.\n",
            "  Batch   280  of    385.    Elapsed: 0:47:31.\n",
            "  Batch   320  of    385.    Elapsed: 0:54:09.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RenMjthRCDJY"
      },
      "source": [
        "# torch.save(model,'model.pth')\n",
        "# model = torch.load('model.pth')\n",
        "# Load the dataset into a pandas dataframe.\n",
        "def func_call_test(epoch_id):\n",
        "    test_df = pd.read_csv(\"/content/drive/MyDrive/HASOC/Data/en_Hasoc2021_test_task1.csv\", delimiter=',') #, header=None, names=['id', 'sentence', 'label']\n",
        "    # data.task_1 = data.task_1.str.replace('NOT','0')\n",
        "    # data.task_1 = data.task_1.str.replace('HOF','1')\n",
        "    # data.task_1 = data.task_1.astype(int)\n",
        "    test_sentences = test_df.sentence.values\n",
        "    test_labels = test_df.label.values\n",
        "    # Report the number of sentences.\n",
        "    print('Number of test sentences: {:,}\\n'.format(test_df.shape[0]))\n",
        "    # /Check This\n",
        "    # Will this make sense now?\n",
        "    # Since the files are no longer loading with the gives headers. Think about it and give the necessary loading names\n",
        "    # /\n",
        "    # /test_sentences = test_df.sentence.values ##\n",
        "    # /test_labels = test_df.label.values\n",
        "    # / UNcomment/delete after done\n",
        "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "    test_input_ids = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in test_sentences:\n",
        "        # `encode` will:\n",
        "        #   (1) Tokenize the sentence.\n",
        "        #   (2) Prepend the `[CLS]` token to the start.\n",
        "        #   (3) Append the `[SEP]` token to the end.\n",
        "        #   (4) Map tokens to their IDs.\n",
        "        encoded_sent = tokenizer.encode(str(sent),add_special_tokens = True,)\n",
        "        \n",
        "        test_input_ids.append(encoded_sent)\n",
        "\n",
        "    # Pad our input tokens\n",
        "    test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, \n",
        "                              dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    # Create attention masks\n",
        "    test_attention_masks = []\n",
        "\n",
        "    # Create a mask of 1s for each token followed by 0s for padding\n",
        "    for seq in test_input_ids:\n",
        "      seq_mask = [float(i>0) for i in seq]\n",
        "      test_attention_masks.append(seq_mask) \n",
        "\n",
        "    # Convert to tensors.\n",
        "    prediction_inputs = torch.tensor(test_input_ids).to(torch.int64)\n",
        "    prediction_masks = torch.tensor(test_attention_masks)\n",
        "    prediction_labels = torch.tensor(test_labels).to(torch.int64)\n",
        "\n",
        "    # Set the batch size.  \n",
        "    batch_size = 8  \n",
        "\n",
        "    # Create the DataLoader.\n",
        "    prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "    prediction_sampler = SequentialSampler(prediction_data)\n",
        "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "    # Prediction on test set\n",
        "\n",
        "    print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "    # Put model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    predictions , true_labels = [], []\n",
        "   \n",
        "    # Predict \n",
        "    for batch in prediction_dataloader:\n",
        "      # Add batch to GPU\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      \n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_mask, b_labels = batch\n",
        "      \n",
        "      # Telling the model not to compute or store gradients, saving memory and \n",
        "      # speeding up prediction\n",
        "      with torch.no_grad():\n",
        "          # Forward pass, calculate logit predictions\n",
        "          outputs = model(b_input_ids, token_type_ids=None, \n",
        "                          attention_mask=b_input_mask)\n",
        "\n",
        "      logits = outputs[0]\n",
        "\n",
        "      # Move logits and labels to CPU\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      label_ids = b_labels.to('cpu').numpy()\n",
        "      \n",
        "      # Store predictions and true labels\n",
        "      predictions.append(logits)\n",
        "      true_labels.append(label_ids)\n",
        "\n",
        "    print('    DONE.')\n",
        "\n",
        "    print('Positive samples: %d of %d (%.2f%%)' % (test_df.label.sum(), len(test_df.label), (test_df.label.sum() / len(test_df.label) * 100.0)))\n",
        "    with open('/content/drive/MyDrive/HASOC/Data/predictionstrue1.csv', \"w\") as writer:\n",
        "        for i,line in enumerate(predictions):\n",
        "            writer.write(str(line) +\" \" +str(true_labels[i]) + \"\\n\")\n",
        "      \n",
        "    # Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "    # Combine the correct labels for each batch into a single list.\n",
        "    flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "    def evaluate(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Evaluate Precision, Recall, F1 scores between y_true and y_pred\n",
        "        If output_file is provided, scores are saved in this file otherwise printed to std output.\n",
        "        :param y_true: true labels\n",
        "        :param y_pred: predicted labels\n",
        "        :return: list of scores (F1, Recall, Precision, ExactMatch)\n",
        "        \"\"\"\n",
        "        \n",
        "        assert len(y_true) == len(y_pred)\n",
        "        precision, recall, f1, _ = metrics.precision_recall_fscore_support(y_true, y_pred, labels=[0, 1], average='weighted')\n",
        "        scores = [\n",
        "            \"F1: %f\\n\" % f1,\n",
        "            \"Recall: %f\\n\" % recall,\n",
        "            \"Precision: %f\\n\" % precision,\n",
        "            \"ExactMatch: %f\\n\" % -1.0\n",
        "        ]\n",
        "        for s in scores:\n",
        "            print(s, end='')\n",
        "\n",
        "    # Evaluate predictions    \n",
        "    evaluate(flat_true_labels, flat_predictions)\n",
        "\n",
        "    print('Writing predictions to file...')\n",
        "    with open(\"/content/drive/MyDrive/HASOC/Data/statout\"+epoch_i+\".csv\", \"w\") as writer:\n",
        "      for line in flat_predictions:\n",
        "            writer.write(str(line) + \"\\n\")\n",
        "    #//Check this    \n",
        "   #// You had to interchange these two\n",
        "    # Save predictions to file\n",
        "    with open('/content/drive/MyDrive/HASOC/Data/predictions1.csv', \"w\") as writer:\n",
        "        for line in flat_predictions:\n",
        "            writer.write(str(line) + \"\\n\")\n",
        "            \n",
        "    print('Done writing predictions...')\n",
        "\n",
        "\n",
        "    if(save_model):\n",
        "        v = 0\n",
        "        folder_name = \"Bert-large-cased-\" + str(v)\n",
        "        if(os.path.exists(folder_name)):\n",
        "            v+=1\n",
        "            folder_name = \"Bert-large-uncased-\" + str(v)\n",
        "        else:\n",
        "            os.mkdir(folder_name)\n",
        "\n",
        "        # output_model_file = \"./{}/my_own_model_file.bin\".format(folder_name)\n",
        "        # output_config_file = \"./{}/my_own_config_file.bin\".format(folder_name)\n",
        "        # output_vocab_file = \"./{}/my_own_vocab_file.bin\".format(folder_name)\n",
        "        output_model_file = \"/content/drive/MyDrive/HASOC/model/my_own_model_file.bin\".format(model)\n",
        "        output_config_file = \"/content/drive/MyDrive/HASOC/model/my_own_config_file.bin\".format(model)\n",
        "        output_vocab_file = \"/content/drive/MyDrive/HASOC/model/my_own_vocab_file.bin\".format(model)\n",
        "\n",
        "        # Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
        "\n",
        "        # If we have a distributed model, save only the encapsulated model\n",
        "        # (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
        "        model_to_save = model.module if hasattr(model, 'module') else model\n",
        "\n",
        "        torch.save(model_to_save.state_dict(), output_model_file)\n",
        "        model_to_save.config.to_json_file(output_config_file)\n",
        "        tokenizer.save_vocabulary(output_vocab_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjNF1jq9nf5o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z-WpdHBnMBo"
      },
      "source": [
        "\n",
        "# Create sentence and label lists\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5RYNnBVvqUw"
      },
      "source": [
        "######################## Ignore this cell #####################################\n",
        "\n",
        "# # tokenizer\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# tokenizer = Tokenizer(num_words=100, lower= 1, oov_token=\"<OOV>\")\n",
        "# tokenizer.fit_on_texts(sentences)\n",
        "# word_index = tokenizer.word_index\n",
        "# print(word_index)\n",
        "# #{'<OOV>': 1, 'i': 2, 'to': 3, 'play': 4, 'no': 5, 'want': 6, 'go': 7, 'out': 8, 'like': 9, 'eating': 10}\n",
        "\n",
        "# sequences = tokenizer.texts_to_sequences(sentences)\n",
        "# print(sentences)\n",
        "# print(word_index)\n",
        "# print(sequences)\n",
        "\n",
        "# # pre padding\n",
        "# pre_pad = pad_sequences(sequences, padding='pre')\n",
        "# print(\"\\nword_index = \", word_index)\n",
        "# print(\"\\nsequences = \", sequences)\n",
        "# print(\"\\npadded_seq = \" )\n",
        "# print(pre_pad)\n",
        "\n",
        "# # post padding\n",
        "# post_pad = pad_sequences(sequences, padding='post')\n",
        "# print(\"\\nword_index = \", word_index)\n",
        "# print(\"\\nsequences = \", sequences)\n",
        "# print(\"\\npadded_seq = \" )\n",
        "# print(post_pad)\n",
        "\n",
        "# import seaborn as sns\n",
        "# ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
        "\n",
        "# plt.title('MCC Score per Batch')\n",
        "# plt.ylabel('MCC Score (-1 to +1)')\n",
        "# plt.xlabel('Batch #')\n",
        "\n",
        "# plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jg47NqZDew-"
      },
      "source": [
        "### New section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_tWZG0nDkEL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "cc8f551c-548c-4074-f062-6711b46bbc19"
      },
      "source": [
        "###############################################################\n",
        "\n",
        "# baseline code for task 1\n",
        "# using BERT-based classification\n",
        "# With the exception of the evaluation part \n",
        "# (which reflects the tasks evaluation code), \n",
        "# this code is taken from\n",
        "# https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
        "\n",
        "###############################################################\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import os\n",
        "import os.path\n",
        "from sklearn import metrics\n",
        "import neptune\n",
        "load_model = False\n",
        "save_model = False\n",
        "\n",
        "if load_model:\n",
        "    output_model_file = \"/content/drive/MyDrive/HASOC/model/my_own_model_file.bin\"\n",
        "    output_config_file = \"/content/drive/MyDrive/HASOC/model/my_own_config_file.bin\"\n",
        "    output_vocab_file = \"/content/drive/MyDrive/HASOC/model/my_own_vocab_file.bin\"\n",
        "data.task_1 = data.task_1.str.replace('NOT','0')\n",
        "data.task_1 = data.task_1.str.replace('HOF','1')\n",
        "data.task_1 = data.task_1.astype(int)\n",
        "\n",
        "df2 = data.copy()\n",
        "df = df2.sample(frac=0.8, random_state=0)\n",
        "dev_df = df2.drop(df.index)\n",
        "\n",
        "# Report the number of sentences.\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Get the lists of sentences and their labels.\n",
        "sentences = df.text.values\n",
        "labels = df.task_1.values\n",
        "labels\n",
        "\n",
        "# Get the lists of sentences and their labels.\n",
        "dev_sentences = dev_df.text.values\n",
        "dev_labels = dev_df.task_1.values\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "if load_model:\n",
        "    tokenizer = BertTokenizer.from_pretrained(output_vocab_file, do_lower_case = False)\n",
        "else:\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "dev_input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(str(sent),add_special_tokens = True,)\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "# For every sentence...\n",
        "for sent in dev_sentences:\n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(str(sent),add_special_tokens = True,)\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    dev_input_ids.append(encoded_sent)\n",
        "\n",
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "\n",
        "# Set the maximum sequence length.\n",
        "# I've chosen 64 somewhat arbitrarily. It's slightly larger than the\n",
        "# maximum training sentence length of 47...\n",
        "MAX_LEN = 120\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
        "# as opposed to the beginning.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "dev_input_ids = pad_sequences(dev_input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')\n",
        "\n",
        "# Create attention masks\n",
        "attention_masks = []\n",
        "dev_attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)\n",
        "\n",
        "# For each sentence...\n",
        "for sent in dev_input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    dev_attention_masks.append(att_mask)\n",
        "    \n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs = input_ids \n",
        "validation_inputs = dev_input_ids\n",
        "train_labels = labels\n",
        "validation_labels = dev_labels\n",
        "\n",
        "# Do the same for the masks.\n",
        "train_masks =  attention_masks\n",
        "validation_masks = dev_attention_masks\n",
        "                                             \n",
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs).to(torch.int64)\n",
        "validation_inputs = torch.tensor(validation_inputs).to(torch.int64)\n",
        "\n",
        "train_labels = torch.tensor(train_labels).to(torch.int64)\n",
        "validation_labels = torch.tensor(validation_labels).to(torch.int64)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "if load_model:\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top.\n",
        "    config = BertConfig.from_json_file(output_config_file)\n",
        "    model = BertForSequenceClassification(config)\n",
        "    state_dict = torch.load(output_model_file)\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "else: \n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "        num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                        # You can increase this for multi-class tasks.   \n",
        "        output_attentions = False, # Whether the model returns attentions weights.\n",
        "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "    )\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "#model.cuda()\n",
        "print(device)\n",
        "\n",
        "\n",
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "print('==== Embedding Layer ====\\n')\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "    \n",
        "learning_rate = 2e-5\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = learning_rate, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 4\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "use_neptune = 1                                            \n",
        "if use_neptune:\n",
        "    api = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI4NDRkYTliMS1lZGRjLTRhOWMtOGU0ZC04OTEzNzU1Y2E2Y2MifQ==\"\n",
        "    neptune.init(project_qualified_name='hnlp.hasoc/nlp', api_token=api)\n",
        "    PARAMS = {'lr': learning_rate,}\n",
        "    tags = ['bert']\n",
        "    neptune.create_experiment(\"First_exp\", tags=tags, params=PARAMS)\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "    \n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "    \n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "criterion = nn.CrossEntropyLoss(weight = torch.Tensor([1/3,2/3]).to(device))\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "    #new\n",
        "    train_loss = []\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device).long()\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        logits = outputs[0]\n",
        "        loss = criterion(logits,b_labels)\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "        neptune.log_metric(\"Loss_Train\",loss.item())\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        #new\n",
        "        train_loss.append(loss.item())\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    #print(\"  training loss: {0:.2f}\".format(train_loss))\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "        neptune.log_metric(\"Acc_Val\",tmp_eval_accuracy)\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "    def func_call_test(epoch_id):\n",
        "      #test_df = pd.read_csv(\"/content/drive/MyDrive/HASOC/Data/en_Hasoc2021_test_task1.csv\", delimiter=',') #, header=None, names=['id', 'sentence', 'label']\n",
        "      test_df = pd.read_csv(\"/content/drive/MyDrive/HASOC/Data/en_Hasoc2021_test_task1.csv\", delimiter=',', header=None, names=['id', 'sentence', 'label'])\n",
        "      # data.task_1 = data.task_1.str.replace('NOT','0')\n",
        "      # data.task_1 = data.task_1.str.replace('HOF','1')\n",
        "      # data.task_1 = data.task_1.astype(int)\n",
        "      test_sentences = test_df.sentence.values\n",
        "      test_labels = test_df.label.values\n",
        "      # Report the number of sentences.\n",
        "      print('Number of test sentences: {:,}\\n'.format(test_df.shape[0]))\n",
        "      # /Check This\n",
        "      # Will this make sense now?\n",
        "      # Since the files are no longer loading with the gives headers. Think about it and give the necessary loading names\n",
        "      # /\n",
        "      # /test_sentences = test_df.sentence.values ##\n",
        "      # /test_labels = test_df.label.values\n",
        "      # / UNcomment/delete after done\n",
        "      # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "      test_input_ids = []\n",
        "\n",
        "      # For every sentence...\n",
        "      for sent in test_sentences:\n",
        "          # `encode` will:\n",
        "          #   (1) Tokenize the sentence.\n",
        "          #   (2) Prepend the `[CLS]` token to the start.\n",
        "          #   (3) Append the `[SEP]` token to the end.\n",
        "          #   (4) Map tokens to their IDs.\n",
        "          encoded_sent = tokenizer.encode(str(sent),add_special_tokens = True,)\n",
        "          \n",
        "          test_input_ids.append(encoded_sent)\n",
        "\n",
        "      # Pad our input tokens\n",
        "      test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, \n",
        "                                dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "      # Create attention masks\n",
        "      test_attention_masks = []\n",
        "\n",
        "      # Create a mask of 1s for each token followed by 0s for padding\n",
        "      for seq in test_input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        test_attention_masks.append(seq_mask) \n",
        "\n",
        "      # Convert to tensors.\n",
        "      prediction_inputs = torch.tensor(test_input_ids).to(torch.int64)\n",
        "      prediction_masks = torch.tensor(test_attention_masks)\n",
        "      prediction_labels = torch.tensor(test_labels).to(torch.int64)\n",
        "\n",
        "      # Set the batch size.  \n",
        "      batch_size = 8  \n",
        "\n",
        "      # Create the DataLoader.\n",
        "      prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n",
        "      prediction_sampler = SequentialSampler(prediction_data)\n",
        "      prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n",
        "\n",
        "      # Prediction on test set\n",
        "\n",
        "      print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "\n",
        "      # Put model in evaluation mode\n",
        "      model.eval()\n",
        "\n",
        "      # Tracking variables \n",
        "      predictions , true_labels = [], []\n",
        "    \n",
        "      # Predict \n",
        "      for batch in prediction_dataloader:\n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and \n",
        "        # speeding up prediction\n",
        "        with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions\n",
        "            outputs = model(b_input_ids, token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Store predictions and true labels\n",
        "        predictions.append(logits)\n",
        "        true_labels.append(label_ids)\n",
        "\n",
        "      print('    DONE.')\n",
        "\n",
        "      print('Positive samples: %d of %d (%.2f%%)' % (test_df.label.sum(), len(test_df.label), (test_df.label.sum() / len(test_df.label) * 100.0)))\n",
        "      with open('/content/drive/MyDrive/HASOC/Data/predictionstrue1.csv', \"w\") as writer:\n",
        "          for i,line in enumerate(predictions):\n",
        "              writer.write(str(line) +\" \" +str(true_labels[i]) + \"\\n\")\n",
        "        \n",
        "      # Combine the predictions for each batch into a single list of 0s and 1s.\n",
        "      flat_predictions = [item for sublist in predictions for item in sublist]\n",
        "      flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "      # Combine the correct labels for each batch into a single list.\n",
        "      flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "\n",
        "      def evaluate(y_true, y_pred):\n",
        "          \"\"\"\n",
        "          Evaluate Precision, Recall, F1 scores between y_true and y_pred\n",
        "          If output_file is provided, scores are saved in this file otherwise printed to std output.\n",
        "          :param y_true: true labels\n",
        "          :param y_pred: predicted labels\n",
        "          :return: list of scores (F1, Recall, Precision, ExactMatch)\n",
        "          \"\"\"\n",
        "          \n",
        "          assert len(y_true) == len(y_pred)\n",
        "          precision, recall, f1, _ = metrics.precision_recall_fscore_support(y_true, y_pred, labels=[0, 1], average='weighted')\n",
        "          scores = [\n",
        "              \"F1: %f\\n\" % f1,\n",
        "              \"Recall: %f\\n\" % recall,\n",
        "              \"Precision: %f\\n\" % precision,\n",
        "              \"ExactMatch: %f\\n\" % -1.0\n",
        "          ]\n",
        "          for s in scores:\n",
        "              print(s, end='')\n",
        "\n",
        "      # Evaluate predictions    \n",
        "      evaluate(flat_true_labels, flat_predictions)\n",
        "\n",
        "      print('Writing predictions to file...')\n",
        "      with open(\"/content/drive/MyDrive/HASOC/Data/statout\"+str(epoch_i)+\".csv\", \"w\") as writer:\n",
        "        for line in flat_predictions:\n",
        "              writer.write(str(line) + \"\\n\")\n",
        "      #//Check this    \n",
        "    #// You had to interchange these two\n",
        "      # Save predictions to file\n",
        "      with open('/content/drive/MyDrive/HASOC/Data/predictions1.csv', \"w\") as writer:\n",
        "          for line in flat_predictions:\n",
        "              writer.write(str(line) + \"\\n\")\n",
        "              \n",
        "      print('Done writing predictions...')\n",
        "\n",
        "\n",
        "      if(save_model):\n",
        "          v = 0\n",
        "          folder_name = \"Bert-large-cased-\" + str(v)\n",
        "          if(os.path.exists(folder_name)):\n",
        "              v+=1\n",
        "              folder_name = \"Bert-large-uncased-\" + str(v)\n",
        "          else:\n",
        "              os.mkdir(folder_name)\n",
        "\n",
        "          # output_model_file = \"./{}/my_own_model_file.bin\".format(folder_name)\n",
        "          # output_config_file = \"./{}/my_own_config_file.bin\".format(folder_name)\n",
        "          # output_vocab_file = \"./{}/my_own_vocab_file.bin\".format(folder_name)\n",
        "          output_model_file = \"/content/drive/MyDrive/HASOC/model/my_own_model_file.bin\".format(model)\n",
        "          output_config_file = \"/content/drive/MyDrive/HASOC/model/my_own_config_file.bin\".format(model)\n",
        "          output_vocab_file = \"/content/drive/MyDrive/HASOC/model/my_own_vocab_file.bin\".format(model)\n",
        "\n",
        "          # Step 1: Save a model, configuration and vocabulary that you have fine-tuned\n",
        "\n",
        "          # If we have a distributed model, save only the encapsulated model\n",
        "          # (it was wrapped in PyTorch DistributedDataParallel or DataParallel)\n",
        "          model_to_save = model.module if hasattr(model, 'module') else model\n",
        "\n",
        "          torch.save(model_to_save.state_dict(), output_model_file)\n",
        "          model_to_save.config.to_json_file(output_config_file)\n",
        "          tokenizer.save_vocabulary(output_vocab_file)\n",
        "\n",
        "    func_call_test(epoch_i)\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# torch.save(model,'model.pth')\n",
        "# model = torch.load('model.pth')\n",
        "# Load the dataset into a pandas dataframe.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8f05cb2c788a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# Bring in subpackages.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# from tensorflow.python import keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtable_from_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimization_options\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptimizationOptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_example_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetching_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy_to_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetching_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprefetch_to_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/experimental/ops/parsing_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_experimental_dataset_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mragged\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mragged_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/parsing_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_parsing_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;31m# go/tf-wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# pylint: disable=wildcard-import,undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/parsing_config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mragged\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mragged_math_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mragged\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mragged_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ragged/ragged_math_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_ragged_math_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mragged\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mragged_functional_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/map_fn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mag_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mautograph_ctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConversionOptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoGraphError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/core/converter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyct\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyct\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mast_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyct\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/pyct/anno.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# pylint:disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m# pylint:enable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gast/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNodeVisitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNodeTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_fields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gast/gast.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0m_make_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdescr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gast/gast.py\u001b[0m in \u001b[0;36m_make_node\u001b[0;34m(Name, Fields, Attributes, Bases)\u001b[0m\n\u001b[1;32m     31\u001b[0m             type(Name,\n\u001b[1;32m     32\u001b[0m                  \u001b[0mBases\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                  {'__init__': create_node}))\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EADqISCK7jeM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
